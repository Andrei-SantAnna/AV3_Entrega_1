# Atividade Individual 01 (AV3) - Sistemas DistribuÃ­dos

Este projeto implementa um ambiente distribuÃ­do composto por mÃºltiplas instÃ¢ncias de serviÃ§o (trÃªs containers backend) que sÃ£o balanceadas por um proxy reverso Nginx.

O objetivo principal Ã© entender e demonstrar na prÃ¡tica como o balanceamento de carga contribui para o **desempenho** (distribuiÃ§Ã£o de requisiÃ§Ãµes), **disponibilidade** (tolerÃ¢ncia a falhas) e **escalabilidade** do sistema.

## ğŸ›ï¸ Arquitetura do Ambiente

O ambiente Ã© orquestrado utilizando Docker Compose e consiste nos seguintes serviÃ§os:

  * **`proxy` (Nginx):** Atua como o **Proxy Reverso** e **Balanceador de Carga**. Ele Ã© o Ãºnico ponto de entrada para o cliente (escutando na porta `80`) e distribui as requisiÃ§Ãµes recebidas para os serviÃ§os backend.
  * **`web1` (FastAPI):** InstÃ¢ncia 1 do serviÃ§o backend, executando em um container Docker.
  * **`web2` (FastAPI):** InstÃ¢ncia 2 do serviÃ§o backend, executando em um container Docker.
  * **`web3` (FastAPI):** InstÃ¢ncia 3 do serviÃ§o backend, executando em um container Docker.

Cada serviÃ§o backend possui um Ãºnico endpoint (`/`) que retorna um JSON com informaÃ§Ãµes sobre o servidor que atendeu a requisiÃ§Ã£o, o timestamp, a latÃªncia simulada e o IP do cliente.

## Tecnologias Utilizadas

  * **Docker e Docker Compose:** Para criar, gerenciar e orquestrar os contÃªineres de aplicaÃ§Ã£o.
  * **Nginx:** Utilizado como proxy reverso e balanceador de carga.
  * **FastAPI (Python):** Framework web usado para criar os serviÃ§os backend.

Para executar este projeto, vocÃª precisarÃ¡ de:

  * Docker
  * Docker Compose (geralmente incluÃ­do no Docker Desktop)
  * `curl` ou um navegador web (para testes manuais)
  * `ab` (Apache Benchmark) para testes de carga (Ex: `sudo apt install apache2-utils`)
  * `jq` (para filtrar a saÃ­da JSON da inspeÃ§Ã£o de rede - opcional, mas recomendado): `sudo apt install jq`

## Como Executar

1.  Clone este repositÃ³rio para sua mÃ¡quina local.

2.  Abra um terminal na pasta raiz do projeto.

3.  Execute o seguinte comando para construir as imagens e iniciar todos os contÃªineres:

    ```bash
    docker compose up --build -d
    ```

4.  Uma vez que os contÃªineres estejam em execuÃ§Ã£o, vocÃª pode acessar o serviÃ§o atravÃ©s do proxy em:
    `http://localhost`

##  Como Verificar os IPs dos ContÃªineres (Opcional)

Para fins de depuraÃ§Ã£o, pode ser Ãºtil ver os IPs internos que o Docker atribuiu a cada contÃªiner.

1.  Primeiro, descubra o nome da rede criada pelo Docker Compose:

    ```bash
    docker network ls
    # O nome serÃ¡ algo como "nome-da-pasta-do-projeto_default"
    # Ex: entrega_1_default
    ```

2.  Use o comando `docker network inspect` e filtre a saÃ­da com `jq` para mostrar apenas o nome e o IP de cada serviÃ§o:

    ```bash
    # Substitua 'entrega_1_default' pelo nome da sua rede
    docker network inspect entrega_1_default | jq '.[0].Containers | .[] | {name: .Name, ip: .IPv4Address}'
    ```

3.  O resultado serÃ¡ parecido com este, mostrando os IPs da rede interna:

    ```json
    {
      "name": "entrega_1-web1-1",
      "ip": "172.18.0.3/16"
    }
    {
      "name": "entrega_1-web2-1",
      "ip": "172.18.0.4/16"
    }
    {
      "name": "entrega_1-web3-1",
      "ip": "172.18.0.2/16"
    }
    {
      "name": "entrega_1-proxy-1",
      "ip": "172.18.0.5/16"
    }
    ```

## AnÃ¡lise dos Experimentos

Os testes a seguir foram realizados para analisar o comportamento dos diferentes algoritmos de balanceamento e a resiliÃªncia do sistema.

**Para alterar o algoritmo de balanceamento:**

1.  Pare o ambiente: `docker compose down`
2.  Edite o arquivo `proxy/nginx.conf`.
3.  Na seÃ§Ã£o `upstream backend_servers`, comente ou descomente a diretiva desejada (`least_conn;` ou `ip_hash;`).
4.  Reinicie o ambiente: `docker compose up --build -d`

-----

### Experimento 1: Round Robin (PadrÃ£o)

  * **ConfiguraÃ§Ã£o:** Nenhuma diretiva de algoritmo foi especificada no bloco `upstream`, utilizando o padrÃ£o Round Robin do Nginx.

  * **Teste:** MÃºltiplas requisiÃ§Ãµes foram enviadas ao `http://localhost` (atualizando o navegador) e um teste de carga foi executado com `ab -n 1000 -c 50 http://localhost/`.

  * **Coleta de Dados (Logs):** Os logs do Nginx foram coletados com `docker compose logs proxy`.

  ![Logs_Round_Robin](https://github.com/user-attachments/assets/6b04a406-473b-4e5b-931c-17154a06238b)

  

* **AnÃ¡lise:**
1000 requisiÃ§Ãµes 

![Round_Robin_1000_requisiÃ§Ãµes](https://github.com/user-attachments/assets/881b0ffd-6ede-4b72-95ba-8259e9eb7d8c)

10000 requisiÃ§Ãµes

![Round_Robin_10000_requisiÃ§Ãµes](https://github.com/user-attachments/assets/affb945a-8974-435d-8efe-2189df3c966e)

100000 requisiÃ§Ãµes

![Round_Robin_100000_requisiÃ§Ãµes](https://github.com/user-attachments/assets/d04763bd-2ef0-445c-9095-795a6ef27e9f)

Conforme observado nos logs e nos testes de navegador, o algoritmo Round Robin distribuiu as requisiÃ§Ãµes de forma sequencial e equitativa entre os trÃªs servidores (web1, web2, web3, web1...). A distribuiÃ§Ã£o de carga foi uniforme, com cada servidor recebendo aproximadamente 333 das 1000 requisiÃ§Ãµes, 3333 das 10000 requisiÃ§Ãµes e 33333 das 100000 requisiÃ§Ãµes dos testes de carga ab.

-----

### Experimento 2: Least Connections

  * **ConfiguraÃ§Ã£o:** A diretiva `least_conn;` foi descomentada no `nginx.conf`.

  * **Teste:** Teste de carga com `ab -n 1000 -c 50 http://localhost/`.

  * **AnÃ¡lise:**

1000 requisiÃ§Ãµes 

![learn_connection_1000_requisiÃ§Ãµes](https://github.com/user-attachments/assets/f5590662-d0f0-4f06-8c70-c3adb0e4d629)


10000 requisiÃ§Ãµes


![learn_connection_10000_requisiÃ§Ãµes](https://github.com/user-attachments/assets/91cef213-44fd-48e3-9583-60ee54a86cb3)

100000 requisiÃ§Ãµes

![learn_connection_100000_requisiÃ§Ãµes](https://github.com/user-attachments/assets/332cba2a-7543-4f14-a018-690eb3c1f230)

    O algoritmo `least_conn` envia novas requisiÃ§Ãµes para o servidor com o menor nÃºmero de conexÃµes ativas no momento. Como nossos endpoints tÃªm uma latÃªncia simulada muito curta e uniforme, o comportamento foi muito similar ao Round Robin. Em um cenÃ¡rio real com requisiÃ§Ãµes de duraÃ§Ã£o variÃ¡vel, este algoritmo seria mais eficiente, prevenindo que um servidor fique sobrecarregado enquanto outros estÃ£o ociosos.*

-----

### Experimento 3: IP Hash

  * **ConfiguraÃ§Ã£o:** A diretiva `ip_hash;` foi descomentada no `nginx.conf`.

  * **Teste:** MÃºltiplas requisiÃ§Ãµes enviadas *a partir do mesmo navegador* e `curl` (mesmo IP de cliente).

  * **Coleta de Dados (Logs):**

    ```log
    [COLE SEU LOG DO IP HASH AQUI]
    Exemplo:
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.3:8000 (web2)
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.3:8000 (web2)
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.3:8000 (web2)
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.3:8000 (web2)
    ```

  * **AnÃ¡lise:**
    [INSIRA SUA ANÃLISE AQUI]
    *Exemplo: Este algoritmo direciona o cliente sempre ao mesmo servidor com base no hash do seu IP. Como esperado, todas as requisiÃ§Ãµes do meu navegador e `curl` foram consistentemente roteadas para o mesmo servidor (`web2` no meu teste). Isso demonstra como o `ip_hash` Ã© usado para manter a afinidade de sessÃ£o (session stickiness), o que Ã© vital para aplicaÃ§Ãµes que armazenam estado (como um carrinho de compras).*

-----

### Experimento 4: Teste de TolerÃ¢ncia a Falhas

  * **ConfiguraÃ§Ã£o:** Ambiente executando com o balanceamento Round Robin.

  * **Teste:** SimulaÃ§Ã£o da indisponibilidade de um dos serviÃ§os backend.

    1.  O ambiente estava operando normalmente, distribuindo entre web1, web2 e web3.
    2.  O serviÃ§o `web2` foi parado manualmente: `docker compose stop web2`
    3.  Novas requisiÃ§Ãµes foram enviadas para `http://localhost`.

  * **Coleta de Dados (Logs):**

    ```log
    [COLE SEU LOG DO TESTE DE FALHA AQUI]
    Exemplo (apÃ³s a falha):
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.4:8000 (web1)
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.2:8000 (web3)
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.4:8000 (web1)
    ... "GET / HTTP/1.1" -> 200 ... -> 172.28.0.2:8000 (web3)
    ```

  * **AnÃ¡lise:**
    [INSIRA SUA ANÃLISE AQUI]
    *Exemplo: Imediatamente apÃ³s parar o container `web2`, o Nginx detectou que o servidor nÃ£o estava respondendo e automaticamente o removeu do pool de balanceamento. O serviÃ§o em `http://localhost` continuou funcionando perfeitamente, sem qualquer erro para o cliente. Os logs confirmam que o trÃ¡fego passou a ser distribuÃ­do apenas entre os servidores saudÃ¡veis (`web1` e `web3`). Isso demonstra a capacidade do proxy reverso de garantir alta disponibilidade.*

## ğŸ¬ VÃ­deo de DemonstraÃ§Ã£o

[COLE O LINK PÃšBLICO PARA O SEU VÃDEO DE 2 MINUTOS AQUI]

## ğŸ‘¨â€ğŸ’» Autor

[SEU NOME COMPLETO]
