## Discente - Andrei Boulhosa de Sant'Anna

# Atividade Individual 01 (AV3) - Sistemas Distribu√≠dos

Este projeto implementa um ambiente distribu√≠do composto por m√∫ltiplas inst√¢ncias de servi√ßo (tr√™s containers backend) que s√£o balanceadas por um proxy reverso Nginx.

O objetivo principal √© entender e demonstrar na pr√°tica como o balanceamento de carga contribui para o **desempenho** (distribui√ß√£o de requisi√ß√µes), **disponibilidade** (toler√¢ncia a falhas) e **escalabilidade** do sistema.

## üèõÔ∏è Arquitetura do Ambiente

O ambiente √© orquestrado utilizando Docker Compose e consiste nos seguintes servi√ßos:

  * **`proxy` (Nginx):** Atua como o **Proxy Reverso** e **Balanceador de Carga**. Ele √© o √∫nico ponto de entrada para o cliente (escutando na porta `80`) e distribui as requisi√ß√µes recebidas para os servi√ßos backend.
  * **`web1` (FastAPI):** Inst√¢ncia 1 do servi√ßo backend, executando em um container Docker.
  * **`web2` (FastAPI):** Inst√¢ncia 2 do servi√ßo backend, executando em um container Docker.
  * **`web3` (FastAPI):** Inst√¢ncia 3 do servi√ßo backend, executando em um container Docker.

Cada servi√ßo backend possui um √∫nico endpoint (`/`) que retorna um JSON com informa√ß√µes sobre o servidor que atendeu a requisi√ß√£o, o timestamp, a lat√™ncia simulada e o IP do cliente.

## Tecnologias Utilizadas

  * **Docker e Docker Compose:** Para criar, gerenciar e orquestrar os cont√™ineres de aplica√ß√£o.
  * **Nginx:** Utilizado como proxy reverso e balanceador de carga.
  * **FastAPI (Python):** Framework web usado para criar os servi√ßos backend.

Para executar este projeto, voc√™ precisar√° de:

  * Docker
  * Docker Compose (geralmente inclu√≠do no Docker Desktop)
  * `curl` ou um navegador web (para testes manuais)
  * `ab` (Apache Benchmark) para testes de carga (Ex: `sudo apt install apache2-utils`)
  * `jq` (para filtrar a sa√≠da JSON da inspe√ß√£o de rede - opcional, mas recomendado): `sudo apt install jq`

## Como Executar

1.  Clone este reposit√≥rio para sua m√°quina local.

2.  Abra um terminal na pasta raiz do projeto.

3.  Execute o seguinte comando para construir as imagens e iniciar todos os cont√™ineres:

    ```bash
    docker compose up --build -d
    ```

4.  Uma vez que os cont√™ineres estejam em execu√ß√£o, voc√™ pode acessar o servi√ßo atrav√©s do proxy em:
    `http://localhost`

##  Como Verificar os IPs dos Cont√™ineres (Opcional)

Para fins de depura√ß√£o, pode ser √∫til ver os IPs internos que o Docker atribuiu a cada cont√™iner.

1.  Primeiro, descubra o nome da rede criada pelo Docker Compose:

    ```bash
    docker network ls
    # O nome ser√° algo como "nome-da-pasta-do-projeto_default"
    # Ex: entrega_1_default
    ```

2.  Use o comando `docker network inspect` e filtre a sa√≠da com `jq` para mostrar apenas o nome e o IP de cada servi√ßo:

    ```bash
    # Substitua 'entrega_1_default' pelo nome da sua rede
    docker network inspect entrega_1_default | jq '.[0].Containers | .[] | {name: .Name, ip: .IPv4Address}'
    ```

3.  O resultado ser√° parecido com este, mostrando os IPs da rede interna:

    ```json
    {
      "name": "entrega_1-web1-1",
      "ip": "172.18.0.4/16"
    }
    {
      "name": "entrega_1-web2-1",
      "ip": "172.18.0.2/16"
    }
    {
      "name": "entrega_1-web3-1",
      "ip": "172.18.0.3/16"
    }
    {
      "name": "entrega_1-proxy-1",
      "ip": "172.18.0.5/16"
    }
    ```

## An√°lise dos Experimentos

Os testes a seguir foram realizados para analisar o comportamento dos diferentes algoritmos de balanceamento e a resili√™ncia do sistema.

**Para alterar o algoritmo de balanceamento:**

1.  Pare o ambiente: `docker compose down`
2.  Edite o arquivo `proxy/nginx.conf`.
3.  Na se√ß√£o `upstream backend_servers`, comente ou descomente a diretiva desejada (`least_conn;` ou `ip_hash;`).
4.  Reinicie o ambiente: `docker compose up --build -d`

-----

### Experimento 1: Round Robin (Padr√£o)

  * **Configura√ß√£o:** Nenhuma diretiva de algoritmo foi especificada no bloco `upstream`, utilizando o padr√£o Round Robin do Nginx.

  * **Teste:** M√∫ltiplas requisi√ß√µes foram enviadas ao `http://localhost` (atualizando o navegador) e um teste de carga foi executado com `ab -n 1000 -c 50 http://localhost/`.

  * **Coleta de Dados (Logs):** Os logs do Nginx foram coletados com `docker compose logs proxy`.

  ![Logs_Round_Robin](https://github.com/user-attachments/assets/6b04a406-473b-4e5b-931c-17154a06238b)

  

* **An√°lise:**
1000 requisi√ß√µes 

![Round_Robin_1000_requisi√ß√µes](https://github.com/user-attachments/assets/881b0ffd-6ede-4b72-95ba-8259e9eb7d8c)

10000 requisi√ß√µes

![Round_Robin_10000_requisi√ß√µes](https://github.com/user-attachments/assets/affb945a-8974-435d-8efe-2189df3c966e)

100000 requisi√ß√µes

![Round_Robin_100000_requisi√ß√µes](https://github.com/user-attachments/assets/d04763bd-2ef0-445c-9095-795a6ef27e9f)

Conforme observado nos logs e nos testes de navegador, o algoritmo Round Robin distribuiu as requisi√ß√µes de forma sequencial e equitativa entre os tr√™s servidores (web1, web2, web3, web1...). A distribui√ß√£o de carga foi uniforme, com cada servidor recebendo aproximadamente 333 das 1000 requisi√ß√µes, 3333 das 10000 requisi√ß√µes e 33333 das 100000 requisi√ß√µes dos testes de carga ab.

-----

### Experimento 2: Least Connections

  * **Configura√ß√£o:** A diretiva `least_conn;` foi descomentada no `nginx.conf`.

  * **Teste:** Teste de carga com `ab -n 1000 -c 50 http://localhost/`.
  * **Coleta de Dados (Logs):**
  
![Logs_Leans_conn](https://github.com/user-attachments/assets/5a0e8491-3ef6-4964-bd74-ab48207b2fbb)

  * **An√°lise:**

1000 requisi√ß√µes 

![learn_connection_1000_requisi√ß√µes](https://github.com/user-attachments/assets/f5590662-d0f0-4f06-8c70-c3adb0e4d629)


10000 requisi√ß√µes


![learn_connection_10000_requisi√ß√µes](https://github.com/user-attachments/assets/91cef213-44fd-48e3-9583-60ee54a86cb3)

100000 requisi√ß√µes

![learn_connection_100000_requisi√ß√µes](https://github.com/user-attachments/assets/332cba2a-7543-4f14-a018-690eb3c1f230)

    O algoritmo `least_conn` envia novas requisi√ß√µes para o servidor com o menor n√∫mero de conex√µes ativas no momento. Como nossos endpoints t√™m uma lat√™ncia simulada muito curta e uniforme, o comportamento foi muito similar ao Round Robin. Em um cen√°rio real com requisi√ß√µes de dura√ß√£o vari√°vel, este algoritmo seria mais eficiente, prevenindo que um servidor fique sobrecarregado enquanto outros est√£o ociosos.*

-----

### Experimento 3: IP Hash

  * **Configura√ß√£o:** A diretiva `ip_hash;` foi descomentada no `nginx.conf`.

  * **Teste:** M√∫ltiplas requisi√ß√µes enviadas *a partir do mesmo navegador* e `curl` (mesmo IP de cliente).

  * **Coleta de Dados (Logs):**

    ![Logd_IP_Hash](https://github.com/user-attachments/assets/07bf2cd2-88e0-47f9-b7fd-27cf3f06a0dd)

  * **An√°lise:**
1000 requisi√ß√µes 

![IP_Hash_1000_requisi√ß√µes](https://github.com/user-attachments/assets/b9b8ff76-f34f-4054-875c-620975291da1)

10000 requisi√ß√µes

![IP_Hash_10000_requisi√ß√µes](https://github.com/user-attachments/assets/ec848bc9-e520-4317-adad-b20114998ef7)

100000 requisi√ß√µes

![IP_Hash_100000_requisi√ß√µes](https://github.com/user-attachments/assets/c0c52ff1-fbca-4a6a-b7c9-95142c5dc188)


  Este algoritmo direciona o cliente sempre ao mesmo servidor com base no hash do seu IP. Como esperado, todas as requisi√ß√µes do meu navegador e `curl` foram consistentemente roteadas para o mesmo servidor (`web1` no meu teste). Isso demonstra como o `ip_hash` √© usado para manter a afinidade de sess√£o (session stickiness), o que √© vital para aplica√ß√µes que armazenam estado (como um carrinho de compras).*

-----

### Experimento 4: Teste de Toler√¢ncia a Falhas

  * **Configura√ß√£o:** Ambiente executando com o balanceamento Round Robin.

  * **Teste:** Simula√ß√£o da indisponibilidade de um dos servi√ßos backend.

    1.  O ambiente estava operando normalmente, distribuindo entre web1, web2 e web3.
    2.  O servi√ßo `web2` foi parado manualmente: `docker compose stop web2`
    3.  Novas requisi√ß√µes foram enviadas para `http://localhost`.

  * **Coleta de Dados (Logs):**

  * Log

![logs_teste_sem_web2](https://github.com/user-attachments/assets/4c2ba221-0031-431b-9afc-3b81e647db2d)

    

  * **An√°lise:**

    * Imediatamente ap√≥s parar o container `web2`, o Nginx detectou que o servidor n√£o estava respondendo e automaticamente o removeu do pool de balanceamento. O servi√ßo em `http://localhost` continuou funcionando perfeitamente, sem qualquer erro para o cliente. Os logs confirmam que o tr√°fego passou a ser distribu√≠do apenas entre os servidores saud√°veis (`web1` e `web3`). Isso demonstra a capacidade do proxy reverso de garantir alta disponibilidade.*

## üé¨ V√≠deo de Demonstra√ß√£o

[COLE O LINK P√öBLICO PARA O SEU V√çDEO DE 2 MINUTOS AQUI]

